---
title: CNAPI (Compute Node API) Design
apisections: <%= sections.map(function (s) { return s.name }).join(", ") %>
markdown2extras: tables, code-friendly
---
<!--
    This Source Code Form is subject to the terms of the Mozilla Public
    License, v. 2.0. If a copy of the MPL was not distributed with this
    file, You can obtain one at http://mozilla.org/MPL/2.0/.
-->

<!--
    Copyright (c) 2014, Joyent, Inc.
-->

<!-- WARNING: index.md is generated from docs/index/index.md.ejs.
    Make you edits to the latter. -->


# Overview

CNAPI is the 'Compute Node API' which presents an API to communicate and
interact with Compute Nodes (CNs).

# Responsibilities

CNAPI provides a unified interface to common Compute Node operations, such as
server setup, factory-resetting, virtual machine life-cycle actions (creation,
state transitions, destruction, etc.) In general, if it needs to talk to
compute nodes, it should happen through CNAPI.

# Compute Node Startup

When a compute node is started up from a shutdown state, regardless if it has
been set up, it will broadcast a message containing the payload from the
sysinfo utility. This broadcast message is picked up by CNAPI.

# Configuration

Reference for configuration variables in cnapi, which are stored in
config/config.json in a running setup. An example of this configuration can be
found in sapi_manifests/cnapi/template.

| Var                       | Type   | Default | Description                                                         |
| ------------------------- | ------ | ------- | ------------------------------------------------------------------- |
| **logLevel**              | String | info    | Level at which to log. One of the supported Bunyan log levels.      |
| **datacenter_name**       | String | -       | Name of the SDC datacenter on which CNAPI is running.               |
| **adminUuid**             | String | -       | The UUID of the admin user in this SDC standup.                     |
| **amqp**                  | Object | -       | If either transport above specifies "amqp", this section is needed. |
| **amqp.host**             | String | -       | Host of AMQP broker.                                                |
| **moray.host**            | String | -       | The Moray API URL.                                                  |
| **moray.port**            | Number | 2020    | The Moray API port.                                                 |
| **api.port**              | Number | 80      | Port number on which to listen.                                     |
| **wfapi.workflows**       | Array  | []      | Array of workflows to load.                                         |
| **wfapi.url**             | String | -       | The Workflow API URL.                                               |
| **napi.url**              | String | -       | The NAPI API URL.                                                   |
| **assets.url**            | String | -       |                                                                     |
| **cnapi.url**             | String | -       | The CNAPI API URL (e.g. of this instance)                           |
| **imgapi.url**            | String | -       | The IMGAPI API URL.                                                 |
| **dapi.changeDefaults**   | Object | -       | This provides some means to override VM allocation behaviour.       |
| **dapi.changeDefaults.server_spread**        | String | min-ram      | How VMs are spread across CNs (one of: min-ram, max-ram, min-owner, and random)   |
| **dapi.changeDefaults.filter_headnode**      | String | true         | Whether VMs cannot allocate on the headnode.                             |
| **dapi.changeDefaults.filter_min_resources** | String | true         | Whether CPU/RAM/disk limits are ignored when allocating.                 |
| **dapi.changeDefaults.filter_large_servers** | String | true         | Whether large servers are reserved for larger allocations.               |
| **dapi.allocationDescription**               | Array  | see template | The pipeline used by the allocator to decide where a VM goes across CNs. |

dapi.changeDefaults is a bit of an oddball, due to limitations in the hogan.js
template engine. Booleans are represented by the "true" and "false" strings, not
raw booleans; an empty string is treated as the default value. Be careful when
changing from the defaults in production.

# SAPI Configuration

When using the config-agent service in the CNAPI zone, which draws metadata from
SAPI, it's possible to change the dapi.changeDefaults outlined in the
`Configuration` section above.

In the SAPI application named `sdc`, adding or changing the following keys in
`metadata` will affect allocation behaviour. This is useful for testing, or
specialized circumstances in production.

| Key                            | Type    | Default | Description                                                                  |
| ------------------------------ | ------- | ------- | ---------------------------------------------------------------------------- |
| **ALLOC_SERVER_SPREAD**        | String  | min-ram | How the allocator spreads VMs across CNs.                                    |
| **ALLOC_FILTER_HEADNODE**      | Boolean | true    | Whether the headnode should be removed from consideration during allocation. |
| **ALLOC_FILTER_MIN_DISK**      | Boolean | false   | Whether CNs with insufficient spare disk should be removed.                  |
| **ALLOC_FILTER_MIN_RESOURCES** | Boolean | true    | Whether CNs with insufficient spare CPU/RAM/disk should be removed.          |
| **ALLOC_FILTER_LARGE_SERVERS** | Boolean | true    | Whether large servers should be reserved primarily for large allocations.    |
| **ALLOC_FILTER_VM_COUNT**      | Integer | null    | If set, CNs with more VMs than the count will be removed from consideration. |

If any of the keys above aren't in the `sdc` `metadata` section, it's treated as
if the default value was specified. Be careful when changing from the default
values in production.

ALLOC_SERVER_SPREAD is of particular interest to certain specialised production
installs. It can have one of four values: `min-ram`, `max-ram`, `min-owner`,
and `random`.  `min-ram` selects CNs which have the least amount of sufficient
space for a new VM; this is desirable to keep emptier servers free for larger
allocations.  `max-ram` selects CNs which have the *most* amount of free
space; this can be desirable for private SDCs to give VMs the currently least
busy servers. `min-owner` makes the allocator much more aggressive about
balancing all VMs belonging to one user across all CNs. And `random` assigns
randomly across CNs.

A note of warning about ALLOC_FILTER_MIN_DISK: if this is set to true, but
ALLOC_FILTER_MIN_RESOURCES is set to false, then disk checks will be ignored.
Both must be true for disk checks to proceed.

### Example

    cnapi_svc=$(sdc-sapi /services?name=cnapi | json -Ha uuid)
    sdc-sapi /services/$cnapi_svc -X PUT -d '{ "metadata": { "ALLOC_FILTER_HEADNODE": false } }'

# Interacting with CNAPI

There are two ways of interacting with CNAPI. Indirectly: eg. adminui, cloudapi,
workflow, vmapi. Directly: eg. sdc-cnapi, sdc-server, curl.

Use it as so:

    -bash-4.1# sdc-cnapi /servers/5e4bafa8-9dfd-11e3-982d-a7dee2e79ac4 \
                    -X POST \
                    -d '{ "datacenter_name": "foo" }'


# Heartbeats

Each compute node is populated with services which allow the headnode to
monitor usage and interact with the compute nodes in general. One of these is
the "heartbeater" agent, its responsibility is to periodically emit server and
zone information to AMQP. CNAPI is connects to AMQP and listens for these
heartbeat messages from all servers. It uses the periodic action of these
heartbeats to determine whether a compute node is up.

If a compute node is not setup (and therefore has no agents besides ur), CNAPI
uses the frequency of the sysinfo messages sent by Ur.


# Resetting to Factory Defaults

To reset a compute node to its factory default state, `PUT` to the server's
`factory-reset` endpoint:

    -bash-4.1# sdc-cnapi /servers/564d5f0d-3517-5f60-78f1-ce6d0b8f58df/factory-reset \
                    -X PUT
    HTTP/1.1 202 Accepted
    Content-Type: application/json
    Content-Length: 51
    Date: Tue, 25 Feb 2014 09:24:52 GMT
    Server: Compute Node API
    x-request-id: ae6426e0-9dfe-11e3-96ca-d3493bcec4fe
    x-response-time: 28
    x-server-name: a6b7ba97-deb7-44b1-85da-3d7ae328c710
    Connection: keep-alive

    {
      "job_uuid": "4a664491-aa29-4d77-9fc2-592308d56922"
    }

The UUID of the factory reset job is returned and can be used to poll for the
completion of the operation.


# Virtual Machine Actions

One of the main mechanisms via which CNAPI interacts with compute nodes VMs is via
AMQP messages sent to and received from the provisioner agent on the compute
node.


    -bash-4.1# sdc-cnapi /servers/$(sysinfo |json UUID)/vms/f9940090-a065-11e3-81fd-274008e46b67machine_reboot \
            -X POST \
            -d '{}'

See the reference for the API for available VM endpoitns.



# Remote Execution

CNAPI exposes a mechanism to allow remote execution of commands.

    -bash-4.1# sdc-cnapi /servers/$(sysinfo |json UUID)/execute \
            -X POST \
            -d '{ "script": "#!/bin/bash\necho hi $1 $FOO", "args": ["hello"],
                  "env": { "FOO": "1" } }'

Using the script, args, and env properties we can control the source we
execute, the arguments to that script and any environment variables.


# Boot parameters

When a compute node boots up, its boot-loader fetches the necessary information
from booter. These booter in turn requests this data, consisting of
`boot_platform`, `kernel_flags` and `kernel_modules` from CNAPI.

Operations on boot parameters are done via the `/boot` endpoint.

On the the initial, boot from a "factory default" state, the "default" boot
parameters will be fetched from the `/boot/default` endpoint.

Setting the default boot platform for new compute nodes:

    -bash-4.1# sdc-cnapi /boot/ac586cae-9ace-11e3-a64e-7f4008875a90 \
        -X PUT \
        -d '{ "boot_platform": "20140219T205617Z" }'


Kernel arguments are key/value pairs passed in to the kernel. They are distinct
from kernel flags.

For example, to set the kernel arguments and flags for a compute node with uuid
21306a50-9dad-11e3-9404-53f0c3de6cb8:

    -bash-4.1# sdc-cnapi /boot/21306a50-9dad-11e3-9404-53f0c3de6cb8 \
        -X POST
        -d '{ "kernel_args": { "foo": "bar" }, "kernel_flags": { "-k": true } }'


Passing `null` as the value to a key deletes that key/value.

For instance, to delete the `foo` key:

    sdc-cnapi /boot/21306a50-9dad-11e3-9404-53f0c3de6cb8 \
        -X POST
        -d '{ "kernel_args": { "foo": null } }'


To completely overwrite values, use PUT instead of POST:

    -bash-4.1# sdc-cnapi /boot/21306a50-9dad-11e3-9404-53f0c3de6cb8 \
        -X PUT
        -d '{ "kernel_args": { "alpha": "able" } }'


# Setting up a new Server

Setting when a new server comes online its `status` should be be visible as
'running', and its `setup` state should be `'unsetup'`:

    -bash-4.1# sdc-cnapi /servers | json -Hga uuid setup status
    564d2cec-76f9-2438-7f66-9140267bed05 true running
    564d5f0d-3517-5f60-78f1-ce6d0b8f58df false running


To set up the new server, one may use one of the indirect methods (adminui,
etc).

Additionally, one may also use `sdc-server`:

    -bash-4.1# sdc-server setup 564d5f0d-3517-5f60-78f1-ce6d0b8f58df


Or, sdc-cnapi:

    -bash-4.1# sdc-cnapi /servers/564d5f0d-3517-5f60-78f1-ce6d0b8f58df/setup \
                    -X PUT


# Updating Nics

The only parameter of the server's nics that can be changed is
nic_tags_provided. This parameter can be changed depending on the following
values for the *action* parameter:

* update: Add nic tags to the target nics
* replace: Replace the nic tags (ie: completely overwrite the list) for the
  target nics
* delete: Remove the nic tags from the target nics

## Examples

**update**

Add the manta nic tag to a nic with sdc-cnapi:

    sdc-cnapi /servers/564d4d2c-ddd0-7be7-40ae-bae473a1d53e/nics \
    -X PUT '\
        {
            "action": "update",
            "nics": [
                {
                    "mac": "00:0c:29:a1:d5:3e",
                    "nic_tags_provided": [ "manta" ]
                }
            ]
        }'

Or with sdc-server:

    sdc-server update-nictags -s 564d4d2c-ddd0-7be7-40ae-bae473a1d53e \
        manta_nic=00:0c:29:a1:d5:3e

**replace**

Set the nic tags for a nic to external and mantanat (removing all other nic
tags) with sdc-cnapi:

    sdc-cnapi /servers/564d4d2c-ddd0-7be7-40ae-bae473a1d53e/nics \
    -X PUT '\
        {
            "action": "replace",
            "nics": [
                {
                    "mac": "00:0c:29:a1:d5:3e",
                    "nic_tags_provided": [ "external", "mantanat" ]
                }
            ]
        }'

Or with sdc-server:

    sdc-server replace-nictags -s 564d4d2c-ddd0-7be7-40ae-bae473a1d53e \
        external_nic=00:0c:29:a1:d5:3e mantanat_nic=00:0c:29:a1:d5:3e

**delete**

Remove the mantanat nic tag from a nic with sdc-cnapi:

    sdc-cnapi /servers/564d4d2c-ddd0-7be7-40ae-bae473a1d53e/nics \
    -X PUT '\
        {
            "action": "delete",
            "nics": [
                {
                    "mac": "00:0c:29:a1:d5:3e",
                    "nic_tags_provided": [ "mantanat" ]
                }
            ]
        }'

Or with sdc-server:

    sdc-server delete-nictags -s 564d4d2c-ddd0-7be7-40ae-bae473a1d53e \
        mantanat_nic=00:0c:29:a1:d5:3e

# Server records


A CNAPI server record looks like the following

    -bash-4.1# sdc-cnapi /servers
    HTTP/1.1 200 OK
    Content-Type: application/json
    Content-Length: 9848
    Date: Tue, 22 Apr 2014 08:35:10 GMT
    Server: Compute Node API
    x-request-id: 03f340c0-c9f9-11e3-9a2b-e36882367c85
    x-response-time: 15
    x-server-name: c587f0fc-a962-49cb-a4d2-cd9cb0efb9b9
    Connection: keep-alive

    {
      "sysinfo": {
         --- compute node sysinfo ---
      },
      "ram": 4095,
      "current_platform": "20140421T214627Z",
      "headnode": true,
      "boot_platform": "20140421T214627Z",
      "datacenter": "coal",
      "overprovision_ratio": 1,
      "reservation_ratio": 0.15,
      "reservoir": false,
      "traits": {},
      "rack_identifier": "",
      "comments": "",
      "uuid": "564d4374-d703-b97b-ca9f-7375f05f337c",
      "hostname": "headnode",
      "reserved": false,
      "boot_params": {
        "rabbitmq": "guest:guest:rabbitmq.coal.joyent.us:5672"
      },
      "kernel_flags": {},
      "default_console": "vga",
      "serial": "ttyb",
      "setup": true,
      "setting_up": false,
      "last_boot": "2014-04-22T07:39:50.000Z",
      "created": "2014-04-22T07:37:30.000Z",
      "vms": {
         --- compute node vm objects ---
      },
      "transitional_status": "",
      "last_heartbeat": "2014-04-22T08:35:07.776Z",
      "status": "running",
      "memory_available_bytes": 2044813312,
      "memory_arc_bytes": 184096272,
      "memory_total_bytes": 4284993536,
      "memory_provisionable_bytes": -44936986624,
      "disk_kvm_zvol_used_bytes": 0,
      "disk_kvm_zvol_volsize_bytes": 0,
      "disk_kvm_quota_bytes": 0,
      "disk_zone_quota_bytes": 536870912000,
      "disk_cores_quota_bytes": 429496729600,
      "disk_installed_images_used_bytes": 950053376,
      "disk_pool_size_bytes": 159987531776,
      "overprovision_ratios": {
        "ram": 1
      },
      "unreserved_cpu": 400,
      "unreserved_ram": -42863,
      "unreserved_disk": 151669
    }

## Server properties

| Param                                | Type          | Description                                                                |
| ------------------------------------ | ------------- | -------------------------------------------------------------------------- |
| **boot_params**                      | *Object*      |
| **boot_platform**                    | *String*      | The platform image to be booted from on next boot                          |
| **current_platform**                 | *String*      | The platform image currently in use by server                              |
| **comments**                         | *String*      | Description of server                                                      |
| **created**                          | *String date* | Date of server creation                                                    |
| **datacenter**                       | *String*      | Datacenter in which server resides                                         |
| **default_console**                  |               |
| **disk_cores_quota_bytes**           |               |
| **disk_installed_images_used_bytes** |               |
| **disk_kvm_quota_bytes**             |               |
| **disk_kvm_zvol_used_bytes**         |               |
| **disk_kvm_zvol_volsize_bytes**      |               |
| **disk_pool_size_bytes**             |               |
| **disk_zone_quota_bytes**            |               |
| **headnode**                         | *Boolean*     | Whether server is a headnode                                               |
| **hostname**                         | *String*      | Hostname if any                                                            |
| **kernel_flags**                     |               |
| **last_boot**                        |               |
| **last_heartbeat**                   |               |
| **memory_arc_bytes**                 |               |
| **memory_available_bytes**           |               |
| **memory_provisionable_bytes**       |               |
| **memory_total_bytes**               |               |
| **overprovision_ratio**              |               |
| **overprovision_ratios**             |               |
| **rack_identifier**                  |               |
| **ram**                              | *Number*      |                                                                            |
| **reservation_ratio**                |               |
| **reserved**                         | *Boolean*     |                                                                            |
| **reservoir**                        | *Boolean*     |                                                                            |
| **serial**                           | *String*      |                                                                            |
| **setting_up**                       | *Boolean*     | Whether server is in the process of setting up                             |
| **setup**                            | *Boolean*     | Whether server has been marked as set up                                   |
| **status**                           | *Boolean*     | The server's current state of activity                                     |
| **sysinfo**                          | *Object*      | The last given sysinfo payload for server                                  |
| **traits**                           | *Object*      |                                                                            |
| **transport**                        | *String*      | The method via which CNAPI is received updates from CN's. (http or amqp)   |
| **transitional_status**              | *String*      | Takes precedense over `status` when a server is undergoing a status change |
| **unreserved_cpu**                   |               |
| **unreserved_disk**                  |               |
| **unreserved_ram**                   |               |
| **uuid**                             | *String*      | The server's unique identifier                                             |
| **vms**                              | *Object*      | A object representing all the vms on server                                |

# Waitlist

When executing jobs on a server, such vm provision, start, stop, reboot, and
zfs dataset import it is possible that concurrent jobs may interfere with each
other.

To prevent this, a mechanism is required that will queue jobs based on the
type of resource they're acting on. Jobs should be grouped by "scope" and
serialized such that a server can will only execute be executing one job per
scope group at a time. In this way it would be possible to enforce that only
one job be active on a vm on a server, but would still allow jobs to be run
against another vm. Any jobs that come in after one is active will be queued
and dispatch as the previous job finishes.

This system allows for concurrent jobs where the scoping has been set such
that two jobs will not interfere with each other. For instance, two reboot
jobs for two different vms may be run at the same time, however, two reboots
for the same vm will happen in sequential order.

Use of the waitlist is a deliberate process. It is up to the one initiating a job
to create a ticket and wait for it to become active. As such, it is possible
to not use the waitlist at all. However, one then runs the risk of concurrent
jobs trampling each other.

Waitlist tickets are serialized and dispatched one by one according to their
`server_uuid`, `scope` and `id` parameters.

The first step to using the waitlist is to determine the scope and subject (ie
the resource on which the action will be performed). For example this may be
something like 'vm', 'dataset', etc.  This means that the action will be
performed on a resource identified by `id` of the type given by `scope`.

The basic process is as follows: a job starts and it first acquires a ticket
from CNAPI for that particular server and passes in a `scope` and an `id`.

Because waitlist tickets are serviced in order, once a ticket has been created
the next step is to wait for it to become active. This will happen if there
are no tickets for that scope/id combination, or if waited upon and all
preceding tickets are resolved. To find out whether a ticket has become
'active' (ie indicating the job may proceed and do its work), the job may poll
the ticket values, or use the blocking `wait` endpoint for that ticket.

Once the work has been completed, it is up to the job to "release" the ticket,
so that subsequent tickets for that scope/id combination can be serviced.

Acquiring a ticket before performing work is an explicit step (as opposed to
CNAPI doing the serialization internally) in effort to add transparency and to
know what is happening with the SDC pipeline from just looking at the
top-level workflow for some work to be performed.

### Request (create) a ticket

Using the waitlist begins with requesting a ticket. POST to the
CreateWaitlistTicket endpoint. Specify the scope and unique id. An expiry date
must also be specified. Endpoint returns a ticket uuid.

    -bash-4.1# sdc-cnapi /servers/$(sysinfo | json UUID)/tickets -X POST -d '{ "scope": "vm", "id": "nuts", "expires_at": "2015-10-10T00:00:00"}'
    HTTP/1.1 202 Accepted
    Content-Type: application/json
    Content-Length: 47
    Date: Fri, 27 Jun 2014 19:36:47 GMT
    Server: Compute Node API
    x-request-id: 60c72290-fe32-11e3-913f-b11ed03e831d
    x-response-time: 49
    x-server-name: 9d2c3229-1e92-4c3f-98fd-5a7ac5fb28ed
    Connection: keep-alive

    {
      "uuid": "ec8d5ef3-24b6-4582-ade8-c9e9bfb70906"
    }


### Display all tickets

    -bash-4.1# sdc-cnapi /servers/$(sysinfo | json UUID)/tickets
    HTTP/1.1 200 OK
    Content-Type: application/json
    Content-Length: 260
    Date: Fri, 27 Jun 2014 19:37:12 GMT
    Server: Compute Node API
    x-request-id: 6fcc80a0-fe32-11e3-913f-b11ed03e831d
    x-response-time: 14
    x-server-name: 9d2c3229-1e92-4c3f-98fd-5a7ac5fb28ed
    Connection: keep-alive

    [
      {
        "uuid": "ec8d5ef3-24b6-4582-ade8-c9e9bfb70906",
        "server_uuid": "564d6e71-b375-4b81-07ec-ad77fe5fa680",
        "scope": "vm",
        "id": "nuts",
        "expires_at": "2015-10-10T00:00:00",
        "created_at": "2014-06-27T19:36:47.708Z",
        "updated_at": "2014-06-27T19:36:47.708Z",
        "status": "active"
      }
    ]

### Wait on a ticket

    -bash-4.1# sdc-cnapi /tickets/bb5038c2-7498-4e07-b919-df072c76d2dc/wait
    <returns when ticket is released or expires>


### Release a ticket

    -bash-4.1# sdc-cnapi /tickets/bb5038c2-7498-4e07-b919-df072c76d2dc/release -X PUT
    HTTP/1.1 204 No Content
    Date: Fri, 27 Jun 2014 19:42:46 GMT
    Server: Compute Node API
    x-request-id: 3678cb00-fe33-11e3-913f-b11ed03e831d
    x-response-time: 19
    x-server-name: 9d2c3229-1e92-4c3f-98fd-5a7ac5fb28ed
    Connection: keep-alive


### Delete a ticket

Explicitly deletes a waitlist ticket. This will allow the next ticket in line
for the given scope/id to proceed. The next ticket waiting on the same scope/id
will be allowed to proceed.

    -bash-4.1# sdc-cnapi /tickets/ec8d5ef3-24b6-4582-ade8-c9e9bfb70906 -X DELETE
    HTTP/1.1 204 No Content
    Date: Fri, 27 Jun 2014 19:41:14 GMT
    Server: Compute Node API
    x-request-id: ff8dcd70-fe32-11e3-913f-b11ed03e831d
    x-response-time: 14
    x-server-name: 9d2c3229-1e92-4c3f-98fd-5a7ac5fb28ed
    Connection: keep-alive



<!-- Genererated API docs -->
<% sections.forEach(function (section) { %># <%= section.name %>

<% section.endpoints.forEach(function (endpoint) { %>## <%= endpoint.name %> (<%= endpoint.endpoint %>)

<%= endpoint.summary %><% if (endpoint.body) { %>

<%= endpoint.body %><% } %>

### Inputs

<% if (endpoint.params.length) { %><%= makeTable({
    headers: ["Param", "Type", "Description"],
    fields: ["name", "type", "description"],
    data: endpoint.params
}) %><% } else { %>None.<% } %>


### Responses

<% if (endpoint.responses.length) { %><%= makeTable({
    headers: ["Code", "Type", "Description"],
    fields: ["code", "type", "description"],
    data: endpoint.responses
}) %><% } else { %>_None_<% } %>


<% }); %>
<% }); %>

<!-- End of genererated API docs -->
